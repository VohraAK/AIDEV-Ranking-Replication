task_type,agent,rank,composite_score,mean_resolution_days,merge_rate,details
build,Claude_Code,1.0,1.0,0.044237764550264555,1.0,Rank 1 | Composite=1.000 | MRT=0.044 | MergeRate=1.000
build,Copilot,1.0,1.0,2.980592013888889,0.5504587155963303,Rank 1 | Composite=1.000 | MRT=2.981 | MergeRate=0.550
build,OpenAI_Codex,3.0,0.9301763751886989,0.17540967470402954,0.9069148936170213,Rank 3 | Composite=0.930 | MRT=0.175 | MergeRate=0.907
build,Cursor,4.0,0.5742835603564747,0.16887490354938273,0.75,Rank 4 | Composite=0.574 | MRT=0.169 | MergeRate=0.750
build,Devin,5.0,0.32512423107777855,0.2766482107107107,0.5873015873015873,Rank 5 | Composite=0.325 | MRT=0.277 | MergeRate=0.587
chore,OpenAI_Codex,1.0,1.9999999999999998,1.0013062728221178,0.8554216867469879,Rank 1 | Composite=2.000 | MRT=1.001 | MergeRate=0.855
chore,Claude_Code,2.0,1.942068464541905,3.929145833333333,0.8333333333333334,Rank 2 | Composite=1.942 | MRT=3.929 | MergeRate=0.833
chore,Cursor,3.0,1.144457024090668,0.6642476851851852,0.8043478260869565,Rank 3 | Composite=1.144 | MRT=0.664 | MergeRate=0.804
chore,Copilot,4.0,1.0,2.221631102693603,0.47413793103448276,Rank 4 | Composite=1.000 | MRT=2.222 | MergeRate=0.474
chore,Devin,5.0,0.25113721906368736,0.534705261472164,0.5698924731182796,Rank 5 | Composite=0.251 | MRT=0.535 | MergeRate=0.570
ci,Cursor,1.0,1.6344285533229992,0.6383317901234568,0.9375,Rank 1 | Composite=1.634 | MRT=0.638 | MergeRate=0.938
ci,OpenAI_Codex,2.0,1.2007329959767277,0.2863379731156595,0.912,Rank 2 | Composite=1.201 | MRT=0.286 | MergeRate=0.912
ci,Copilot,3.0,1.140776699029126,1.398923335537919,0.65625,Rank 3 | Composite=1.141 | MRT=1.399 | MergeRate=0.656
ci,Devin,4.0,0.7106850730699112,0.713774112654321,0.6101694915254238,Rank 4 | Composite=0.711 | MRT=0.714 | MergeRate=0.610
ci,Claude_Code,5.0,0.5799352750809064,0.010677083333333334,0.8,Rank 5 | Composite=0.580 | MRT=0.011 | MergeRate=0.800
docs,Claude_Code,1.0,1.2145669291338583,1.419413097993827,0.75,Rank 1 | Composite=1.215 | MRT=1.419 | MergeRate=0.750
docs,Devin,2.0,1.1217982862436315,3.383525510097201,0.7269736842105263,Rank 2 | Composite=1.122 | MRT=3.384 | MergeRate=0.727
docs,Copilot,3.0,1.0,3.8194093475219826,0.6967418546365914,Rank 3 | Composite=1.000 | MRT=3.819 | MergeRate=0.697
docs,OpenAI_Codex,3.0,1.0,0.18848135836342883,0.944954128440367,Rank 3 | Composite=1.000 | MRT=0.188 | MergeRate=0.945
docs,Cursor,5.0,0.8337835121959263,0.5069733796296296,0.806282722513089,Rank 5 | Composite=0.834 | MRT=0.507 | MergeRate=0.806
feat,Claude_Code,1.0,1.5668982807692136,1.7125293427230048,0.7029702970297029,Rank 1 | Composite=1.567 | MRT=1.713 | MergeRate=0.703
feat,Cursor,2.0,1.5372921370767774,0.9622074123855038,0.7112810707456979,Rank 2 | Composite=1.537 | MRT=0.962 | MergeRate=0.711
feat,Devin,3.0,1.1366032715018937,1.7623616926104848,0.5665024630541872,Rank 3 | Composite=1.137 | MRT=1.762 | MergeRate=0.567
feat,Copilot,4.0,1.0,3.432961040055087,0.5231788079470199,Rank 4 | Composite=1.000 | MRT=3.433 | MergeRate=0.523
feat,OpenAI_Codex,5.0,0.9999999999999998,0.3228438139585995,0.8403282775815499,Rank 5 | Composite=1.000 | MRT=0.323 | MergeRate=0.840
fix,Claude_Code,1.0,1.6908843914326637,2.928580948372615,0.7252747252747253,Rank 1 | Composite=1.691 | MRT=2.929 | MergeRate=0.725
fix,Cursor,2.0,1.4933209431376684,0.8737362841043891,0.7698630136986301,Rank 2 | Composite=1.493 | MRT=0.874 | MergeRate=0.770
fix,Copilot,3.0,1.2438131027330883,3.381178528380451,0.5423620025673941,Rank 3 | Composite=1.244 | MRT=3.381 | MergeRate=0.542
fix,OpenAI_Codex,4.0,0.9999999999999998,0.5881073069396042,0.8517448856799037,Rank 4 | Composite=1.000 | MRT=0.588 | MergeRate=0.852
fix,Devin,5.0,0.8659192478693849,0.9447731179173576,0.4426094137076796,Rank 5 | Composite=0.866 | MRT=0.945 | MergeRate=0.443
other,Cursor,1.0,2.0,0.0006404320987654322,0.375,Rank 1 | Composite=2.000 | MRT=0.001 | MergeRate=0.375
perf,OpenAI_Codex,1.0,1.1410033567523632,0.2485975669818755,0.734375,Rank 1 | Composite=1.141 | MRT=0.249 | MergeRate=0.734
perf,Copilot,2.0,1.0707510667934288,2.10756462191358,0.3870967741935484,Rank 2 | Composite=1.071 | MRT=2.108 | MergeRate=0.387
perf,Devin,3.0,0.9999999999999999,2.232346380471381,0.36065573770491804,Rank 3 | Composite=1.000 | MRT=2.232 | MergeRate=0.361
perf,Claude_Code,4.0,0.9472945564774757,0.23763310185185185,0.6666666666666666,Rank 4 | Composite=0.947 | MRT=0.238 | MergeRate=0.667
perf,Cursor,5.0,0.5066483893077453,0.12525568181818184,0.55,Rank 5 | Composite=0.507 | MRT=0.125 | MergeRate=0.550
refactor,Cursor,1.0,1.6612625030314128,1.376864872685185,0.7422680412371134,Rank 1 | Composite=1.661 | MRT=1.377 | MergeRate=0.742
refactor,Copilot,2.0,1.0841755905434667,3.3901015142746918,0.5877551020408164,Rank 2 | Composite=1.084 | MRT=3.390 | MergeRate=0.588
refactor,Devin,3.0,1.0435641931167088,0.9712727281212664,0.5904761904761905,Rank 3 | Composite=1.044 | MRT=0.971 | MergeRate=0.590
refactor,OpenAI_Codex,4.0,1.0,0.43421661901475567,0.8329637841832964,Rank 4 | Composite=1.000 | MRT=0.434 | MergeRate=0.833
refactor,Claude_Code,5.0,0.9999999999999999,1.874451566951567,0.5652173913043478,Rank 5 | Composite=1.000 | MRT=1.874 | MergeRate=0.565
revert,Copilot,1.0,2.0,1.1362355324074074,1.0,Rank 1 | Composite=2.000 | MRT=1.136 | MergeRate=1.000
revert,Devin,2.0,0.6186561913305034,0.6691059027777778,0.6666666666666666,Rank 2 | Composite=0.619 | MRT=0.669 | MergeRate=0.667
revert,OpenAI_Codex,3.0,0.40000000000000036,0.1322945601851852,0.8,Rank 3 | Composite=0.400 | MRT=0.132 | MergeRate=0.800
style,Cursor,1.0,1.2658931134215707,0.3400587606837607,0.8666666666666667,Rank 1 | Composite=1.266 | MRT=0.340 | MergeRate=0.867
style,Copilot,2.0,0.9999999999999998,0.9713233024691358,0.42857142857142855,Rank 2 | Composite=1.000 | MRT=0.971 | MergeRate=0.429
style,OpenAI_Codex,3.0,0.9531772575250836,0.11141508838383839,0.8461538461538461,Rank 3 | Composite=0.953 | MRT=0.111 | MergeRate=0.846
style,Devin,4.0,0.5867128007417708,0.12242170479302834,0.68,Rank 4 | Composite=0.587 | MRT=0.122 | MergeRate=0.680
test,Cursor,1.0,1.5381966670345437,1.6916636473429951,0.6764705882352942,Rank 1 | Composite=1.538 | MRT=1.692 | MergeRate=0.676
test,Devin,2.0,1.1742781026083091,1.5493163449466414,0.5315315315315315,Rank 2 | Composite=1.174 | MRT=1.549 | MergeRate=0.532
test,Copilot,3.0,1.0,3.172119763205829,0.4621212121212121,Rank 3 | Composite=1.000 | MRT=3.172 | MergeRate=0.462
test,OpenAI_Codex,4.0,0.9999999999999998,0.2271119589239445,0.8603945371775418,Rank 4 | Composite=1.000 | MRT=0.227 | MergeRate=0.860
test,Claude_Code,5.0,0.48357581407824834,0.33329475308641976,0.6,Rank 5 | Composite=0.484 | MRT=0.333 | MergeRate=0.600
